package service

import (
	"database/sql"
	"fmt"
	"github.com/IceBear-CreditEase-LLM/aigc-admin/src/encode"
	"github.com/IceBear-CreditEase-LLM/aigc-admin/src/repository/types"
	"github.com/IceBear-CreditEase-LLM/aigc-admin/src/util"
	"github.com/go-kit/log"
	"github.com/go-kit/log/level"
	"github.com/google/uuid"
	"github.com/spf13/cobra"
	"golang.org/x/crypto/bcrypt"
	"gorm.io/driver/mysql"
	"gorm.io/gorm"
	"os"
	"strings"
)

var (
	generateCmd = &cobra.Command{
		Use:               "generate command <args> [flags]",
		Short:             "生成命令",
		SilenceErrors:     false,
		DisableAutoGenTag: false,
		Example: `## 生成命令
可用的配置类型：
[table, init-data]

aigc-admin generate -h
`,
	}

	genTableCmd = &cobra.Command{
		Use:               `table <args> [flags]`,
		Short:             "生成数据库表",
		SilenceErrors:     false,
		DisableAutoGenTag: false,
		Example: `
aigc-admin generate table all
`,
		RunE: func(cmd *cobra.Command, args []string) error {
			// 关闭资源连接
			defer func() {
				_ = level.Debug(logger).Log("db", "close", "err", db.Close())
			}()

			if len(args) > 0 && args[0] == "all" {
				return generateTable()
			}
			return nil
		},
		PreRunE: func(cmd *cobra.Command, args []string) error {
			logger = log.NewLogfmtLogger(os.Stdout)
			// 连接数据库
			if strings.EqualFold(dbDrive, "mysql") {
				dbUrl := fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?charset=utf8mb4&parseTime=true&loc=Local&timeout=20m&collation=utf8mb4_unicode_ci",
					mysqlUser, mysqlPassword, mysqlHost, mysqlPort, mysqlDatabase)
				var dbErr error
				sqlDB, err := sql.Open("mysql", dbUrl)
				if err != nil {
					_ = level.Error(logger).Log("sql", "Open", "err", err.Error())
					return err
				}
				gormDB, err = gorm.Open(mysql.New(mysql.Config{
					Conn:              sqlDB,
					DefaultStringSize: 255,
				}), &gorm.Config{
					DisableForeignKeyConstraintWhenMigrating: true,
				})
				if dbErr != nil {
					_ = level.Error(logger).Log("db", "connect", "err", dbErr.Error())
					dbErr = encode.ErrServerStartDbConnect.Wrap(dbErr)
					return dbErr
				}
				//gormDB.Statement.Clauses["soft_delete_enabled"] = clause.Clause{}
				db, dbErr = gormDB.DB()
				if dbErr != nil {
					_ = level.Error(logger).Log("gormDB", "DB", "err", dbErr.Error())
					dbErr = encode.ErrServerStartDbConnect.Wrap(dbErr)
					return dbErr
				}
				_ = level.Debug(logger).Log("mysql", "connect", "success", true)
			}
			return nil
		},
	}
)

func generateTable() (err error) {
	_ = logger.Log("migrate", "table", "Chat", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Chat{}))
	_ = logger.Log("migrate", "table", "ChatAllowUser", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatAllowUser{}))
	_ = logger.Log("migrate", "table", "ChatConversation", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatConversation{}))
	_ = logger.Log("migrate", "table", "ChatSystemPrompt", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatSystemPrompt{}))
	_ = logger.Log("migrate", "table", "ChatPromptTypes", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatPromptTypes{}))
	_ = logger.Log("migrate", "table", "ChatChannels", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatChannels{}))
	_ = logger.Log("migrate", "table", "ChatPrompts", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatPrompts{}))
	_ = logger.Log("migrate", "table", "ChatChannelModels", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatChannelModels{}))
	_ = logger.Log("migrate", "table", "ChatMessages", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ChatMessages{}))
	_ = logger.Log("migrate", "table", "Dataset", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Dataset{}))
	_ = logger.Log("migrate", "table", "DatasetSample", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.DatasetSample{}))
	_ = logger.Log("migrate", "table", "Assistants", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Assistants{}))
	_ = logger.Log("migrate", "table", "Tools", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Tools{}))
	_ = logger.Log("migrate", "table", "AssistantToolAssociations", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.AssistantToolAssociations{}))
	_ = logger.Log("migrate", "table", "Files", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Files{}))
	_ = logger.Log("migrate", "table", "SysAudit", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.SysAudit{}))
	_ = logger.Log("migrate", "table", "FineTuningTrainJob", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.FineTuningTrainJob{}))
	_ = logger.Log("migrate", "table", "FineTuningTemplate", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.FineTuningTemplate{}))
	_ = logger.Log("migrate", "table", "Tenants", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Tenants{}))
	_ = logger.Log("migrate", "table", "Models", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.Models{}))
	_ = logger.Log("migrate", "table", "SysDict", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.SysDict{}))
	_ = logger.Log("migrate", "table", "ModelDeploy", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.ModelDeploy{}))
	_ = logger.Log("migrate", "table", "LLMEvalResults", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.LLMEvalResults{}))
	_ = logger.Log("migrate", "table", "DatasetDocument", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.DatasetDocument{}))
	_ = logger.Log("migrate", "table", "DatasetDocumentSegment", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.DatasetDocumentSegment{}))
	_ = logger.Log("migrate", "table", "DatasetAnnotationTask", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.DatasetAnnotationTask{}))
	_ = logger.Log("migrate", "table", "DatasetAnnotationTaskSegment", gormDB.Set("gorm:table_options", "ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;").AutoMigrate(types.DatasetAnnotationTaskSegment{}))
	//err = initData()
	//if err != nil {
	//	return err
	//}
	return
}

// 初始化数据
func initData() (err error) {
	tenant := types.Tenants{
		Name:           "系统租户",
		PublicTenantID: uuid.New().String(),
		ContactEmail:   serverAdminUser,
	}
	_ = logger.Log("init", "data", "SysDict", gormDB.Create(&tenant).Error)
	password, err := bcrypt.GenerateFromPassword([]byte(serverAdminPass), bcrypt.DefaultCost)
	if err != nil {
		return err
	}
	_ = logger.Log("init", "data", "account", gormDB.Save(&types.Accounts{
		Email:        serverAdminUser,
		Nickname:     "系统管理员",
		Language:     "zh",
		IsLdap:       false,
		PasswordHash: string(password),
		Status:       true,
		Tenants:      []types.Tenants{tenant},
	}).Error)
	if serverChannelKey == "" {
		serverChannelKey = "sk-" + string(util.Krand(48, util.KC_RAND_KIND_ALL))
	}
	_ = logger.Log("init", "data", "ChatChannels", gormDB.Create(&types.ChatChannels{
		Name:       "default",
		Alias:      "默认渠道",
		Remark:     "默认渠道",
		Quota:      10000,
		Models:     "default",
		OnlyOpenAI: false,
		ApiKey:     serverChannelKey,
		Email:      serverAdminUser,
		TenantId:   tenant.ID,
	}).Error)

	_ = logger.Log("init", "data", "sys_dict", gormDB.Exec(initSysDictSql).Error)
	_ = logger.Log("init", "data", "finetuning_template", gormDB.Exec(ftTemplateSql).Error)
	_ = logger.Log("init", "data", "models", gormDB.Exec(ftTemplateSql).Error)
	return err
}

var (
	modelSql = `INSERT INTO models (id, created_at, updated_at, deleted_at, provider_name, model_type, model_name, max_tokens, is_private, is_fine_tuning, enabled, remark, parameters, last_operator, base_model_name, replicas, label, k8s_cluster, inferred_type, gpu, cpu, memory)
VALUES
	(2, '2024-02-04 13:02:48.112', '2024-03-19 14:08:35.667', NULL, 'OpenAI', 'text-generation', 'gpt-3.5-turbo', 4096, 0, 0, 1, 'OpenAI GPT-3.5-turbo', 20.00, '', NULL, 1, NULL, NULL, NULL, 0, 0, 1),
	(3, '2024-03-18 17:34:59.542', '2024-03-19 14:51:35.630', NULL, 'LocalAI', 'text-generation', 'qwen1.5-0.5b', 32768, 1, 0, 0, '', 0.50, 'admin', '', 1, '', '', '', 0, 0, 1),
	(4, '2024-03-19 14:03:11.073', '2024-03-19 14:41:08.194', NULL, 'LocalAI', 'text-generation', 'qwen1.5-1.8b', 32768, 0, 0, 0, '', 1.80, 'admin', '', 1, '', '', '', 0, 0, 1),
	(5, '2024-03-19 14:03:34.619', '2024-03-19 14:41:41.709', NULL, 'LocalAI', 'text-generation', 'qwen1.5-1.8b-chat', 32768, 0, 0, 0, '', 1.80, 'admin', '', 1, '', '', '', 0, 0, 1),
	(6, '2024-03-19 14:03:51.375', '2024-03-19 14:41:36.354', NULL, 'LocalAI', 'text-generation', 'qwen1.5-4b', 32768, 0, 0, 0, '', 3.98, 'admin', '', 1, '', '', '', 0, 0, 1),
	(7, '2024-03-19 14:04:11.425', '2024-03-19 14:41:11.423', NULL, 'LocalAI', 'text-generation', 'qwen1.5-4b-chat', 32768, 0, 0, 0, '', 3.98, 'admin', '', 1, '', '', '', 0, 0, 1),
	(8, '2024-03-19 14:04:29.257', '2024-03-19 14:41:18.790', NULL, 'LocalAI', 'text-generation', 'qwen1.5-7b', 32768, 0, 0, 0, '', 7.20, 'admin', '', 1, '', '', '', 0, 0, 1),
	(9, '2024-03-19 14:04:45.241', '2024-03-19 14:41:24.050', NULL, 'LocalAI', 'text-generation', 'qwen1.5-7b-chat', 32768, 0, 0, 0, '', 7.20, 'admin', '', 1, '', '', '', 0, 0, 1),
	(10, '2024-03-19 14:05:04.519', '2024-03-19 14:41:27.394', NULL, 'LocalAI', 'text-generation', 'qwen1.5-14b', 32768, 0, 0, 0, '', 14.20, 'admin', '', 1, '', '', '', 0, 0, 1),
	(11, '2024-03-19 14:05:27.624', '2024-03-19 14:41:47.741', NULL, 'LocalAI', 'text-generation', 'qwen1.5-14b-chat', 32768, 0, 0, 0, '', 14.20, 'admin', '', 1, '', '', '', 0, 0, 1),
	(12, '2024-03-19 14:06:26.666', '2024-03-19 14:41:33.633', NULL, 'LocalAI', 'text-generation', 'qwen1.5-72b', 32768, 0, 0, 0, '', 72.30, 'admin', '', 1, '', '', '', 0, 0, 1),
	(13, '2024-03-19 14:06:43.121', '2024-03-19 14:41:30.391', NULL, 'LocalAI', 'text-generation', 'qwen1.5-72b-chat', 32768, 0, 0, 0, '', 72.30, 'admin', '', 1, '', '', '', 0, 0, 1),
	(14, '2024-03-19 14:08:27.352', '2024-03-19 14:41:01.068', NULL, 'LocalAI', 'text-generation', 'qwen-plus', 32768, 0, 0, 0, '', 14.20, 'admin', '', 1, '', '', '', 0, 0, 1);
`

	ftTemplateSql = `INSERT INTO fine_tuning_template (id, created_at, updated_at, deleted_at, name, base_model, content, params, train_image, remark, base_model_path, script_file, output_dir, max_tokens, lora, enabled, template_type)
VALUES
	(2, '2024-03-18 17:36:17.927', '2024-03-19 14:34:05.091', NULL, 'qwen1.5-0.5b', 'qwen1.5-0.5b', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-0-5b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(3, '2024-03-19 14:13:04.852', '2024-03-19 14:38:20.662', NULL, 'qwen1.5-0.5b-train', 'qwen1.5-0.5b', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-0-5b', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(4, '2024-03-18 17:36:17.927', '2024-03-19 14:33:55.959', NULL, 'qwen1.5-1.8b', 'qwen1.5-1.8b', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-1-8b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(5, '2024-03-18 17:36:17.927', '2024-03-19 14:33:48.463', NULL, 'qwen1.5-1.8b-chat', 'qwen1.5-1.8b-chat', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-1-8b-chat', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(6, '2024-03-18 17:36:17.927', '2024-03-19 14:33:38.874', NULL, 'qwen1.5-4b', 'qwen1.5-4b', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-4b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(7, '2024-03-18 17:36:17.927', '2024-03-19 14:34:18.717', NULL, 'qwen1.5-4b-chat', 'qwen1.5-4b-chat', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-4b-chat', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(8, '2024-03-18 17:36:17.927', '2024-03-19 14:34:28.766', NULL, 'qwen1.5-7b', 'qwen1.5-7b', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-7b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(9, '2024-03-19 17:36:17.927', '2024-03-19 14:32:46.468', NULL, 'qwen1.5-7b-chat', 'qwen1.5-7b-chat', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-0-5b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(10, '2024-03-19 17:36:17.927', '2024-03-19 14:34:40.884', NULL, 'qwen1.5-14b', 'qwen1.5-14b', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-14b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(11, '2024-03-19 17:36:17.927', '2024-03-19 14:33:01.470', NULL, 'qwen1.5-14b-chat', 'qwen1.5-14b-chat', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-0-5b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(12, '2024-03-19 17:36:17.927', '2024-03-19 14:33:16.820', NULL, 'qwen1.5-72b', 'qwen1.5-72b', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-72b', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(13, '2024-03-19 17:36:17.927', '2024-03-19 14:33:26.291', NULL, 'qwen1.5-72b-chat', 'qwen1.5-72b-chat', '#!/bin/bash\n\nMODEL_WORKER=fastchat.serve.model_worker\nCONTROLLER_ADDRESS=http://fschat-controller:21001\nMODEL_NAME={{.modelName}}\nMODEL_PATH={{.modelPath}}\nHTTP_PORT={{.port}}\nQUANTIZATION={{.quantization}}\nNUM_GPUS={{.numGpus}}\nMAX_GPU_MEMORY={{.maxGpuMemory}}\nVLLM={{.vllm}}\nINFERRED_TYPE={{.inferredType}}\nOS_TYPE=$(uname)\n\n# awq 量化配置\n# awq_wbits\n# awq_groupsize\n\n# 并发限制\n# limit-worker-concurrency\n\n# gptq 量化配置\n# gptq_wbits\n# gptq_groupsize\n# gptq_act_order\n\n# MODEL_WORKER \nif [ \"$VLLM\" == \"true\" ]; then\n    MODEL_WORKER=\"fastchat.serve.vllm_worker\"\nfi\n\n# 量化配置\nif [ \"$QUANTIZATION\" == \"8bit\" ]; then\n    QUANTIZATION=\"--load-8bit\"\nelse\n    QUANTIZATION=\"\"\nfi\n\n# NUM_GPUS\nif [ \"$NUM_GPUS\" -gt 0 ]; then\n    NUM_GPUS=\"--num-gpus $NUM_GPUS\"\nelse\n    NUM_GPUS=\"\"\nfi\n\n# CPU推理CPU，mps\nif [ \"$INFERRED_TYPE\" == \"cpu\" ] && [ \"$OS_TYPE\" == \"Darwin\" ]; then\n    DEVICE_OPTION=\"--device mps\"\nelif [ \"$INFERRED_TYPE\" == \"cpu\" ]; then\n    DEVICE_OPTION=\"--device cpu\"\nelse\n    DEVICE_OPTION=\"\"\nfi\n\n# MAX_GPU_MEMORY\nif [ \"$MAX_GPU_MEMORY\" -gt 0 ]; then\n    MAX_GPU_MEMORY=\"--max-gpu-memory ${MAX_GPU_MEMORY}GiB\"\nelse\n    MAX_GPU_MEMORY=\"\"\nfi\n\npython3 -m $MODEL_WORKER --host 0.0.0.0 --port $HTTP_PORT \\\n    --controller-address $CONTROLLER_ADDRESS \\\n    --worker-address http://$MY_POD_IP:$HTTP_PORT \\\n    --model-name $MODEL_NAME \\\n    --model-path $MODEL_PATH \\\n    $QUANTIZATION $NUM_GPUS $MAX_GPU_MEMORY $DEVICE_OPTION\n', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-72b-chat', '/app/start.sh', '/data/ft-model/', 32768, 0, 1, 'inference'),
	(14, '2024-03-19 14:13:04.852', '2024-03-19 14:38:34.152', NULL, 'qwen1.5-1.8b-train', 'qwen1.5-1.8b', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-1-8b', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(15, '2024-03-19 14:13:04.852', '2024-03-19 14:38:40.229', NULL, 'qwen1.5-1.8b-chat-train', 'qwen1.5-1.8b-chat', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-1-8b-chat', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(16, '2024-03-19 14:13:04.852', '2024-03-19 14:39:11.505', NULL, 'qwen1.5-4b-chat-train', 'qwen1.5-4b-chat', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-4b-chat', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(17, '2024-03-19 14:13:04.852', '2024-03-19 14:39:03.600', NULL, 'qwen1.5-4b-train', 'qwen1.5-4b', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-4b', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(18, '2024-03-19 14:13:04.852', '2024-03-19 14:39:24.104', NULL, 'qwen1.5-7b-train', 'qwen1.5-7b', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model', '/app/finetune.py', '/data/base-model/qwen1-5-7b', 32768, 0, 1, 'train'),
	(19, '2024-03-19 14:13:04.852', '2024-03-19 14:39:34.841', NULL, 'qwen1.5-7b-chat-train', 'qwen1.5-7b-chat', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-7b-chat', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(20, '2024-03-19 14:13:04.852', '2024-03-19 14:39:50.220', NULL, 'qwen1.5-14b-chat-train', 'qwen1.5-14b-chat', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-14b-chat', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(21, '2024-03-19 14:13:04.852', '2024-03-19 14:39:55.548', NULL, 'qwen1.5-14b-train', 'qwen1.5-14b', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-14b', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(22, '2024-03-19 14:13:04.852', '2024-03-19 14:40:05.661', NULL, 'qwen1.5-72b-train', 'qwen1.5-72b', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-72b', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train'),
	(23, '2024-03-19 14:13:04.852', '2024-03-19 14:40:15.699', NULL, 'qwen1.5-72b-chat-train', 'qwen1.5-72b-chat', '#!/bin/bash\nexport AUTH=sk-001\nexport JOB_ID={{.JobId}}\n\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\n\nGPUS_PER_NODE={{.ProcPerNode}}\nNNODES=1\nNODE_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT={{.MasterPort}}\nUSE_LORA={{.Lora}}\nQ_LORA=False\n\nMODEL=\"{{.BaseModelPath}}\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"{{.DataPath}}\"\n# 验证集\nEVAL_DATA=\"{{.ValidationFile}}\"\nDS_CONFIG_PATH=\"ds_config_zero3.json\"\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\nif [ \"$USE_LORA\" == \"true\" ]; then\n    USE_LORA=True\n    DS_CONFIG_PATH=\"ds_config_zero2.json\"\nelse\n    USE_LORA=False\n    DS_CONFIG_PATH=\"ds_config_zero3.json\"\nfi\n\nmkdir -p /data/train-data/\nwget -O {{.DataPath}} {{.FileUrl}}\n\ntorchrun $DISTRIBUTED_ARGS {{.ScriptFile}} \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 True \\\n    --output_dir {{.OutputDir}} \\\n    --num_train_epochs {{.TrainEpoch}} \\\n    --per_device_train_batch_size {{.TrainBatchSize}} \\\n    --per_device_eval_batch_size {{.EvalBatchSize}} \\\n    --gradient_accumulation_steps {{.AccumulationSteps}} \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate {{.LearningRate}} \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length {{.ModelMaxLength}} \\\n    --gradient_checkpointing True \\\n    --lazy_preprocess True \\\n    --use_lora ${USE_LORA} \\\n    --q_lora ${Q_LORA} \\\n    --deepspeed $DS_CONFIG_PATH', '', 'dudulu/qwen-train:v0.2.36-0319', '', '/data/base-model/qwen1-5-72b-chat', '/app/finetune.py', '/data/ft-model/', 32768, 0, 1, 'train');
`

	initSysDictSql = `INSERT INTO sys_dict (id, created_at, updated_at, deleted_at, parent_id, code, dict_value, dict_label, dict_type, sort, remark)
VALUES
	(2, '2023-11-22 16:19:52.000', '2024-01-29 10:32:18.000', NULL, 0, 'speak_gender', 'gender', '性别', 'int', 1, '性别'),
	(3, '2023-11-22 16:23:19.000', '2024-01-29 10:32:18.000', NULL, 2, 'speak_gender', '1', '男', 'int', 1, '性别:男'),
	(4, '2023-11-22 16:24:27.000', '2024-01-29 10:32:18.000', NULL, 2, 'speak_gender', '2', '女', 'int', 0, '性别:女'),
	(5, '2023-11-23 10:17:31.000', '2023-11-30 10:42:43.000', NULL, 0, 'speak_age_group', 'speak_age_group', '年龄段', 'int', 0, ''),
	(6, '2023-11-23 10:18:31.000', '2023-11-23 10:20:51.000', NULL, 5, 'speak_age_group', '1', '少年', 'int', 5, ''),
	(7, '2023-11-23 10:18:46.000', '2023-11-23 10:20:51.000', NULL, 5, 'speak_age_group', '2', '青年', 'int', 4, ''),
	(8, '2023-11-23 10:18:56.000', '2023-11-23 10:20:51.000', NULL, 5, 'speak_age_group', '3', '中年', 'int', 4, ''),
	(9, '2023-11-23 10:19:21.000', '2023-11-23 10:20:51.000', NULL, 5, 'speak_age_group', '4', '老年', 'int', 2, ''),
	(10, '2023-11-23 10:25:07.000', '2023-11-30 10:42:43.000', NULL, 0, 'speak_style', 'speak_style', '风格', 'int', 0, ''),
	(11, '2023-11-23 10:25:53.000', '2023-11-23 10:25:53.000', NULL, 10, 'speak_style', '1', '温柔', 'int', 5, ''),
	(12, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '2', '阳光', 'int', 4, ''),
	(13, '2023-11-23 10:28:24.000', '2023-12-08 14:04:05.000', NULL, 0, 'speak_area', 'speak_area', '适应范围', 'int', 0, ''),
	(14, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '1', '客服', 'int', 5, ''),
	(15, '2023-11-23 10:29:14.000', '2023-11-23 10:29:14.000', NULL, 13, 'speak_area', '2', '小说', 'int', 4, ''),
	(16, '2023-11-23 10:32:39.000', '2023-11-23 10:32:39.000', NULL, 0, 'speak_lang', 'speak_lang', '语言', 'string', 0, ''),
	(17, '2023-11-23 10:33:28.000', '2023-11-23 10:33:28.000', NULL, 16, 'speak_lang', 'zh-CN', '中文（普通话，简体）', 'string', 100, ''),
	(18, '2023-11-23 10:34:08.000', '2023-11-23 10:34:08.000', NULL, 16, 'speak_lang', 'zh-HK', '中文（粤语，繁体）', 'string', 99, ''),
	(19, '2023-11-23 10:34:30.000', '2023-11-23 10:34:30.000', NULL, 16, 'speak_lang', 'en-US', '英语（美国）', 'string', 98, ''),
	(20, '2023-11-23 10:35:07.000', '2023-11-23 10:35:07.000', NULL, 16, 'speak_lang', 'en-GB', '英语（英国）', 'string', 97, ''),
	(21, '2023-11-23 10:44:23.000', '2023-11-23 10:44:23.000', NULL, 0, 'speak_provider', 'speak_provider', '供应商', 'string', 0, ''),
	(22, '2023-11-23 10:44:50.000', '2023-11-23 10:44:50.000', NULL, 21, 'speak_provider', 'azure', '微软', 'string', 0, ''),
	(23, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '3', '自然流畅', 'int', 0, ''),
	(24, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '4', '亲切温和', 'int', 0, ''),
	(25, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '5', '温柔甜美', 'int', 0, ''),
	(26, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '6', '成熟知性', 'int', 0, ''),
	(27, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '7', '大气浑厚', 'int', 0, ''),
	(28, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '8', '稳重磁性', 'int', 0, ''),
	(29, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '9', '年轻时尚', 'int', 0, ''),
	(30, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '10', '轻声耳语', 'int', 0, ''),
	(31, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '11', '可爱甜美', 'int', 0, ''),
	(32, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '12', '呆萌可爱', 'int', 0, ''),
	(33, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '13', '激情力度', 'int', 0, ''),
	(34, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '14', '饱满活泼', 'int', 0, ''),
	(35, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '15', '诙谐幽默', 'int', 0, ''),
	(36, '2023-11-23 10:26:04.000', '2023-11-23 10:26:04.000', NULL, 10, 'speak_style', '16', '淳朴方言', 'int', 0, ''),
	(37, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '3', '新闻', 'int', 0, ''),
	(38, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '4', '纪录片', 'int', 0, ''),
	(39, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '5', '解说', 'int', 0, ''),
	(40, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '6', '教育', 'int', 0, ''),
	(41, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '7', '广告', 'int', 0, ''),
	(42, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '8', '直播', 'int', 0, ''),
	(43, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '9', '助理', 'int', 0, ''),
	(44, '2023-11-23 10:28:57.000', '2023-11-23 10:28:57.000', NULL, 13, 'speak_area', '10', '特色', 'int', 0, ''),
	(45, '2023-11-23 10:34:08.000', '2023-11-24 17:54:17.000', NULL, 16, 'speak_lang', 'zh-CN-henan', '中文（中原官话河南，简体）', 'string', 99, ''),
	(46, '2023-11-23 10:34:08.000', '2023-11-24 17:54:19.000', NULL, 16, 'speak_lang', 'zh-CN-liaoning', '中文（东北官话，简体）', 'string', 99, ''),
	(47, '2023-11-23 10:34:08.000', '2023-11-24 17:54:20.000', NULL, 16, 'speak_lang', 'zh-TW', '中文（台湾普通话，繁体）', 'string', 99, ''),
	(48, '2023-11-23 10:34:08.000', '2023-11-24 17:54:22.000', NULL, 16, 'speak_lang', 'zh-CN-GUANGXI', '中文（广西口音普通话，简体）', 'string', 99, ''),
	(49, '2023-11-23 10:35:07.000', '2023-11-23 10:35:07.000', NULL, 16, 'speak_lang', 'ko-KR', '韩语(韩国)', 'string', 97, ''),
	(50, '2023-11-23 10:35:07.000', '2023-11-24 19:45:54.000', NULL, 16, 'speak_lang', 'ja-JP', '日语（日本）', 'string', 97, ''),
	(51, '2023-11-23 10:35:07.000', '2023-11-24 19:45:54.000', NULL, 16, 'speak_lang', 'fil-PH', '菲律宾语（菲律宾）', 'string', 97, '');
`
)
